
spark on yarn的執行流程以及運行方式的區別(cluster 和 client)

export HADOOP_CONF_DIR=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop

提交ETL spark作業運行的時候，需要傳遞一個day: 20190130
這個時間可以使用date命令來獲取(獲取):
date +"%F"
2019-10-21

[hadoop@hadoop000 ~]$ date +"%F"
2019-10-21
[hadoop@hadoop000 ~]$ date +"%F %H:%M:%S"
2019-10-21 01:18:51
[hadoop@hadoop000 ~]$ date -d"tomorrow"  +"%F %H:%M:%S"
2019-10-22 01:20:27
[hadoop@hadoop000 ~]$ date -d"yesterday"  +"%F %H:%M:%S"
2019-10-20 01:20:43
[hadoop@hadoop000 ~]$ date -d"1 day ago" +"%F %H:%M:%S"
2019-10-20 01:21:37
[hadoop@hadoop000 ~]$ date -d"-1 day ago" +"%F %H:%M:%S"
2019-10-22 01:21:55

**********************************************************
[hadoop@hadoop000 ~]$ date -d"1 day ago" +"%Y%m%d"
20191020
**********************************************************


離線作業，一般是今天凌晨跑昨天的數據(crontab) day-1


spark作業提交

    1、判斷shell腳本傳遞進來的參數是否有一個
    if(args.length != 1){
          println("param error: usage -> ImoocLog <time>")
          System.exit(1)
    }
    2、代碼重構
    3、代碼編譯
        jar傳到服務器環境上
    4、提交腳本
        hbase依赖的jar  jars
        UAParser依赖的jar  packages

        资源不够的时候可以杀掉application  yarn application -kill application_...

        如何使得yarn 申请资源提速

    "spark.serializer","org.apache.spark.serializer.KryoSerializer"

    # Run on a YARN cluster
    export HADOOP_CONF_DIR=XXX

    $SPARK_HOME/bin/spark-submit \
      --class com.kobekun.spark.project.batch.ImoocLog \
      --master yarn \
      --name ImoocLog \
      --conf "spark.serializer=org.apache.spark.serializer.KryoSerializer" \
      --packages cz.mallat.uasparser:uasparser:0.6.2 \
      --jars $(echo $HBASE_HOME/lib/*.jar | tr ' ' ',') \
      /home/hadoop/lib/sparkOverture-1.0-SNAPSHOT.jar \
      20190130

--packages 是需要去中央仓库下载所需要的jar的dependency，生产上的机器不能上网，怎么办？

	  
报下面错解决方案：https://www.cnblogs.com/honeybee/p/6379599.html?utm_source=itdadao&utm_medium=referral

把spark依赖的jar包上传到hadoop上

19/10/20 06:20:08 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
19/10/20 06:20:22 INFO yarn.Client: Uploading resource file:/tmp/spark-fc8d6367-0713-4f71-a6e1-2c4e25976fd0/__spark_libs__6968808471650253166.zip -> hdfs://hadoop001:8020/user/hadoop/.sparkStaging/application_1571577494871_0001/__spark_libs__6968808471650253166.zip
19/10/20 06:20:31 WARN hdfs.DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/hadoop/.sparkStaging/application_1571577494871_0001/__spark_libs__6968808471650253166.zip could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1595)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3287)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:677)
        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:213)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:485)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)

        at org.apache.hadoop.ipc.Client.call(Client.java:1504)
        at org.apache.hadoop.ipc.Client.call(Client.java:1441)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
        at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:425)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:258)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
        at com.sun.proxy.$Proxy13.addBlock(Unknown Source)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1860)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1656)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:790)
19/10/20 06:20:31 INFO yarn.Client: Deleted staging directory hdfs://hadoop001:8020/user/hadoop/.sparkStaging/application_1571577494871_0001
19/10/20 06:20:31 ERROR spark.SparkContext: Error initializing SparkContext.
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/hadoop/.sparkStaging/application_1571577494871_0001/__spark_libs__6968808471650253166.zip could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1595)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3287)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:677)
        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:213)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:485)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)

        at org.apache.hadoop.ipc.Client.call(Client.java:1504)
        at org.apache.hadoop.ipc.Client.call(Client.java:1441)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
        at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:425)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:258)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
        at com.sun.proxy.$Proxy13.addBlock(Unknown Source)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1860)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1656)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:790)
19/10/20 06:20:31 INFO server.AbstractConnector: Stopped Spark@4d4d8fcf{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}


使用du -h -x --max-depth=1  查看哪个目录占用过高，对于过高目录中的内容适当删减腾出一些空间
df -h
