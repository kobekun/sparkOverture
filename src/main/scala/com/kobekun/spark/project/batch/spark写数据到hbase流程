

1、spark读取hdfs上的文件，文件格式或者是spark内置的，或者是自定义数据源类型。
读取成DataFrame。有的字段比如时间字段，时间格式不是要求的格式，可以用spark的UDF函数将
时间格式转换成想要的格式，加到上面的DataFrame中。
2、把DF转换成RDD，其中处理字段时，获取rowkey，把rk放到ImmutableBytesWritable对象中，
列的值设置成KeyValue的形式。后面会保存数据会用到
3、接着需要创建表，hbase表是一天一个，跑作业的时候，需要把时间传进来，创建表需要借助admin，HTableDescriptor来创建。
4、创建完表后，设置作业写数据到哪个表中，这块用到
HFileOutputFormat2.configureIncrementalLoad。意思是配置增量加载数据到指定表中的一个作业。
就是一些加载数据准备工作，比如说把文件加载到缓存中，设置reduce数量匹配region的数量
5、准备工作之后开始保存数据，用RDD.saveAsNewAPIHadoopFile保存数据。这里面的参数有路径，KeyClass(之前
RDD中的ImmutableBytesWritable)，ValueClass(RDD中的KeyValue)，
输出的format类型是HFileOutputFormat2，还有配置。保存完的数据就是HFile文件
6、保存完数据到hdfs上之后，需要把这数据bulkload到刚才创建的表中。才能在hbase上查看到表
7、整个过程会在以脚本并传参的形式跑在yarn上。
流程就是这么个流程。有时候需求变了之后可能得对rdd里面的字段了什么的动手脚











