

流处理系统
    spark streaming
    structured streaming
    flink
    storm
    kafka stream

项目架构及处理流程：
    log => flume => kafka => spark streaming(Direct) ==> redis
    实时：代码来生成订单日志 => kafka => spark streaming(Direct) ==> redis
    离线：hdfs => spark => hbase


大数据团队分工：采集、批处理、实时处理、API、前端


项目需求
    1) 统计每天付费成功的总订单数、订单总金额
    2) 统计每个小时付费成功的总订单数、订单金额
    ==> 统计每分钟付费成功的总订单数、订单金额
    ==> 统计基于window付费成功的总订单数、订单金额
    ==> 付费订单占到总下单的占比：按天、小时、分钟

    sparkstreaming 进行统计分析，分析结果写入到redis



spark streaming&kafka&redis整合
离线日志：访问日志
实时日志：付费日志
    下单，但是没付钱
    下单，付钱
    time,userid,courseid,orderid,fee


1) 启动zk  cd $ZK_HOME/bin  ./zkServer.sh start
2) 启动kafka 到kafka的bin目录下，
./kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties
3) 创建topic
./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1
--partitions 1 --topic kafka_streaming_kobekun_topic
4) 查询topic
./kafka-topics.sh --list --zookeeper localhost:2181
5) 通过控制台测试topic是否能够正常的生产和消费信息
./kafka-console-producer.sh --broker-list localhost:9092 --topic kafka_streaming_kobekun_topic
./kafka-console-consumer.sh --zookeeper localhost:2181 --topic kafka_streaming_kobekun_topic



# 新生产者（支持0.9版本+）
./kafka-console-producer.sh --broker-list localhost:9092 --topic kobekunkafka --producer.config ../config/producer.properties

## 新消费者（支持0.9版本+）
./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic kobekunkafka --new-consumer --from-beginning --consumer.config ../config/consumer.properties

作业：自己根据离线项目的讲解，把实时项目打包到服务器上运行(有坑)

"auto.offset.reset" -> "latest"
spark挂了，kafka还在运行，数据有丢失

kafka offset管理

创建streaming context
从kafka中获取要处理的数据
根据业务处理数据
处理结果入库
启动程序，等待程序终止

挂了：kafka数据到底是从头开始，还是从最新开始

正确的做法：
    第一次应用程序启动的时候，要从某个地方获取到已经消费过的offset
    业务逻辑处理完后，应该要把已经处理完的offset保存到某个地方去

offset 存储的地方
    checkpoint
    kafka
    zk/mysql/hbase/redis

http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#storing-offsets

    Obtaining Offsets

    stream.foreachRDD { rdd =>
      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
      rdd.foreachPartition { iter =>
        val o: OffsetRange = offsetRanges(TaskContext.get.partitionId)
        println(s"${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}")
      }
    }

    Storing Offsets

    stream.foreachRDD { rdd =>
      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges

      // some time later, after outputs have completed
      stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)
    }

 This is why the stream example above sets “enable.auto.commit” to false.
 因为如果数据未入库，而将offset提交之后，数据被标为已消费






